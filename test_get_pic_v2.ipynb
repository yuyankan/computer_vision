{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff3f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee94318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration for screenshots and logs ---\n",
    "SCREENSHOT_DIR = \"screenshots_yolo_generic_detect\"\n",
    "LOG_FILE = \"log_yolo_generic_detect.txt\"\n",
    "\n",
    "# --- Configuration class ---\n",
    "class CFG:\n",
    "    WEIGHTS = 'yolov8n.pt' # Make sure this is your custom model\n",
    "    CONFIDENCE = 0.50      # Confidence threshold for detection\n",
    "    \n",
    "    # Class IDs for detection and reaction (0, 2, 3, 4, 5)\n",
    "    # Make sure these IDs correspond to the desired classes in your model!\n",
    "    # For example, if your model is trained on COCO:\n",
    "    # 0: 'person'\n",
    "    # 2: 'car'\n",
    "    # 3: 'motorcycle'\n",
    "    # 4: 'airplane' (unlikely in a helmet scenario, but for example)\n",
    "    # 5: 'bus'\n",
    "    CLASSES_TO_DETECT = [0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a029bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_event(message): # Renamed from log_alarm for more generality\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] {message}\\n\"\n",
    "    print(log_entry.strip()) # Print to console\n",
    "    log_dir = os.path.dirname(LOG_FILE)\n",
    "    if log_dir and not os.path.exists(log_dir): # Create log directory if it doesn't exist\n",
    "        try:\n",
    "            os.makedirs(log_dir)\n",
    "        except OSError as e:\n",
    "            print(f\"Failed to create log directory: {e}\")\n",
    "    with open(LOG_FILE, \"a\", encoding='utf-8') as f:\n",
    "        f.write(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa7e6995",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_predictions_df_for_videos\u001b[39m(run = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, model = \u001b[43mmodel\u001b[49m, EXP_NAME = CFG.EXP_NAME, video_info = video_properties, save_df = \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m      2\u001b[39m   df = pd.DataFrame()\n\u001b[32m      4\u001b[39m   root = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./runs/detect/predict\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/labels/\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def get_predictions_df_for_videos(run = '', model = model, EXP_NAME = CFG.EXP_NAME, video_info = video_properties, save_df = True):\n",
    "  df = pd.DataFrame()\n",
    "\n",
    "  root = f'./runs/detect/predict{run}/labels/'\n",
    "  pieces = sorted([i for i in os.listdir(root)])\n",
    "\n",
    "  ### iterate over txt files (there is one for each frame in the video)\n",
    "  for i, frame_txt in enumerate([i for i in pieces]):\n",
    "\n",
    "    df = pd.read_csv(root + frame_txt, sep=\" \", header=None) # read txt file as dataframe\n",
    "    df.columns = [\"class\", \"x_center\", \"y_center\", \"width\", \"height\", \"confidence\"] # name columns (detection task)\n",
    "\n",
    "    ### create column 'frame'\n",
    "    frame_number = re.findall(r'\\d+', frame_txt)[-1] # find frame in each txt filename\n",
    "    df['frame'] = [int(frame_number) for i in range(len(df))]\n",
    "\n",
    "    if i == 0:\n",
    "      df_concat = df\n",
    "    else:\n",
    "      df_concat = pd.concat([df_concat, df], axis=0).reset_index(drop=True)\n",
    "\n",
    "  ### create 4 new columns (coordinates converted into pixels): Calculate bounding box coordinates\n",
    "  df_concat['x_min'] = (df_concat['x_center'] * video_info['width']) - ((df_concat['width'] * video_info['width'])/2)\n",
    "  df_concat['x_max'] = (df_concat['x_center'] * video_info['width']) + ((df_concat['width'] * video_info['width'])/2)\n",
    "  df_concat['y_min'] = (df_concat['y_center'] * video_info['height']) - ((df_concat['height'] * video_info['height'])/2)\n",
    "  df_concat['y_max'] = (df_concat['y_center'] * video_info['height']) + ((df_concat['height'] * video_info['height']/2))\n",
    "\n",
    "  ### create 2 new columns: height and width in pixels: will be used to filter out bad predictions (false positives)\n",
    "  df_concat['width_px'] = (df_concat['width'] * video_info['width']).round(0).astype(int)\n",
    "  df_concat['height_px'] = (df_concat['height'] * video_info['height']).round(0).astype(int)\n",
    "\n",
    "  ### sort by frame\n",
    "  df_concat = df_concat.sort_values(by='frame', ascending=True).reset_index(drop=True)\n",
    "\n",
    "  ### add column 'class_name' and rearrange order\n",
    "  df_concat['class_name'] = df_concat['class'].map(model.names)\n",
    "  other_cols = [col for col in df_concat.columns.to_list() if col not in ['frame', 'class_name', 'class', 'confidence' ]]\n",
    "  new_order = ['frame', 'class_name', 'class', 'confidence'] + other_cols\n",
    "  df_concat = df_concat[new_order]\n",
    "\n",
    "  ### export detections df\n",
    "  if save_df:\n",
    "    df_concat.to_csv(CFG.OUTPUT_DIR + f'detections_{CFG.EXP_NAME}_c{CFG.CONFIDENCE_INT}.csv', index=False)\n",
    "\n",
    "  return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd6fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nvr_capture_and_channel():\n",
    "    # --- NVR Configuration ---\n",
    "    nvr_ip_address = \"147.121.119.88\"\n",
    "    rtsp_port = \"554\"\n",
    "    username = \"lgmpublic\" \n",
    "    password = \"Avery@1234\"  \n",
    "    channel_id_val = 12 # Channel to access\n",
    "    stream_type_id = \"01\" # 01 for main stream, 02 for sub stream\n",
    "\n",
    "    rtsp_channel_identifier = str(channel_id_val) + stream_type_id\n",
    "    rtsp_url = f\"rtsp://{username}:{password}@{nvr_ip_address}:{rtsp_port}/Streaming/Channels/{rtsp_channel_identifier}\"\n",
    "    log_event(f\"Attempting to connect to RTSP stream: {rtsp_url.replace(password, '********')}\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(rtsp_url)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        log_event(f\"Error: Failed to open video stream {rtsp_url.replace(password, '********')}\")\n",
    "        return None, None # Return None in case of error\n",
    "\n",
    "    log_event(f\"Video stream for channel {channel_id_val} connected successfully! Press 'q' to exit.\")\n",
    "    return cap, channel_id_val # Return capture object and channel ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f895254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57151cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_logging():\n",
    "    if not os.path.exists(SCREENSHOT_DIR): # Create screenshot directory if it doesn't exist\n",
    "        os.makedirs(SCREENSHOT_DIR)\n",
    "    \n",
    "    try:\n",
    "        model = YOLO(CFG.WEIGHTS)\n",
    "        log_event(f\"YOLO model loaded: {CFG.WEIGHTS}\")\n",
    "        \n",
    "        # Determining device (GPU or CPU)\n",
    "        if torch.cuda.is_available():\n",
    "            target_device_str = 'cuda'\n",
    "            log_event(f\"CUDA available. Model will use GPU ({target_device_str}).\")\n",
    "        else:\n",
    "            target_device_str = 'cpu'\n",
    "            log_event(\"CUDA unavailable. Model will use CPU.\")\n",
    "        \n",
    "        # Checking model class names\n",
    "        if hasattr(model, 'names') and model.names:\n",
    "            log_event(f\"Recognizable classes in model ({CFG.WEIGHTS}): {model.names}\")\n",
    "            # Check if all classes from CLASSES_TO_DETECT are in the model\n",
    "            for class_id_to_check in CFG.CLASSES_TO_DETECT:\n",
    "                if class_id_to_check not in model.names:\n",
    "                    log_event(f\"WARNING: Class with ID {class_id_to_check} is missing from the model {model.names}. It will be ignored.\")\n",
    "        else:\n",
    "            log_event(f\"WARNING: Failed to get class names from model {CFG.WEIGHTS}. Detection will be based on ID only.\")\n",
    "            \n",
    "        return model, target_device_str # Return model and device string\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_event(f\"Error: Failed to load YOLO model '{CFG.WEIGHTS}'. Info: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad07a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variant\n",
    "detected_target_classes_info = []\n",
    "def work_1frame(model,class_names_map, frame, frame_count, process_every_n_frames,channel_id_display, device_str,event_log_cooldown):\n",
    "        \n",
    "    frame_count[0] += 1\n",
    "    display_frame = frame.copy() # Copy frame for display and drawing\n",
    "\n",
    "    if frame_count % process_every_n_frames == 0:\n",
    "        log_event(f\"Processing frame #{frame_count[0]} for channel {channel_id_display}...\")\n",
    "        current_frame_annotations = [] # Annotations for the current frame\n",
    "         # Collect information about detected target classes\n",
    "\n",
    "        # 1. YOLOv8 model prediction\n",
    "        results = model.predict(\n",
    "            source=frame.copy(), # Pass a copy so the original frame remains clean for screenshot\n",
    "            # save=True, # YOLO will save its images if needed - uncomment\n",
    "            classes=CFG.CLASSES_TO_DETECT, # Filter only necessary classes\n",
    "            conf=CFG.CONFIDENCE,\n",
    "            # save_txt=True, # Save annotations to txt if needed - uncomment\n",
    "            # save_conf=True,\n",
    "            show=False, # Set to False, use cv2.imshow for display\n",
    "            device=device_str, # Use determined device\n",
    "            verbose=False # Less console output from YOLO\n",
    "        )\n",
    "        \n",
    "        if results and results[0] and hasattr(results[0], 'boxes') and len(results[0].boxes) > 0:\n",
    "            for i in range(len(results[0].boxes)):\n",
    "                cls_id = int(results[0].boxes.cls[i])\n",
    "                # Since 'classes' arg filters already, all results will be from this list\n",
    "                conf_score = float(results[0].boxes.conf[i])\n",
    "                bbox_xyxy = results[0].boxes.xyxy[i].cpu().numpy().astype(int)\n",
    "                class_name = class_names_map.get(cls_id, f\"ID:{cls_id}\") # Get class name\n",
    "                timestamp_file = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "                info_temp = [channel_id_display,class_name, round(conf_score,2), bbox_xyxy,timestamp_file]\n",
    "\n",
    "                detected_target_classes_info.append(info_temp)\n",
    "                \n",
    "                # Prepare information for drawing\n",
    "                px1, py1, px2, py2 = bbox_xyxy\n",
    "                text_to_draw = f\"{class_name} ({conf_score:.2f})\"\n",
    "                current_frame_annotations.append(\n",
    "                    (text_to_draw, (px1, py1 - 10), \n",
    "                        (px1, py1, px2, py2), \n",
    "                        (0, 0,250), 2) # read color for all target objects\n",
    "                )\n",
    "        \n",
    "        # If target objects were detected in the current processed frame\n",
    "        if detected_target_classes_info:\n",
    "            current_event_time = time.time()\n",
    "            if current_event_time - last_event_log_time[0] > event_log_cooldown:\n",
    "                #timestamp_file = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "                screenshot_filename = os.path.join(SCREENSHOT_DIR, f\"{channel_id_display}_{timestamp_file}.jpg\")\n",
    "                \n",
    "                # To have the screenshot with current detection boxes, draw them on a temp frame before saving\n",
    "                temp_frame_for_screenshot = frame.copy()\n",
    "                for text, text_pos, rect_coords, color, thickness in current_frame_annotations:\n",
    "                        cv2.rectangle(temp_frame_for_screenshot, (rect_coords[0], rect_coords[1]), (rect_coords[2], rect_coords[3]), color, thickness)\n",
    "                        cv2.putText(temp_frame_for_screenshot, text, text_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, thickness)\n",
    "                cv2.imwrite(screenshot_filename, temp_frame_for_screenshot)# save pic\n",
    "                \n",
    "                event_summary = \"; \".join(detected_target_classes_info)\n",
    "                log_message = f\"EVENT (Channel {channel_id_display}): Detected objects - {event_summary}. Screenshot saved: {screenshot_filename}\"\n",
    "                log_event(log_message)\n",
    "                last_event_log_time[0] = current_event_time #change in place\n",
    "                \n",
    "                \n",
    "        \n",
    "        last_drawn_annotations = current_frame_annotations # Update annotations for display\n",
    "    \n",
    "    # Draw saved annotations on the display frame\n",
    "    for text, text_pos, rect_coords, color, thickness in last_drawn_annotations:\n",
    "        cv2.rectangle(display_frame, (rect_coords[0], rect_coords[1]), (rect_coords[2], rect_coords[3]), color, thickness)\n",
    "        cv2.putText(display_frame, text, text_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, thickness)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow(f'Object Detection - NVR Channel {channel_id_display}', display_frame)\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ffc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b860bd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 16:12:41] Attempting to connect to RTSP stream: rtsp://lgmpublic:********@147.121.119.88:554/Streaming/Channels/1201\n",
      "[2025-05-27 16:12:42] Video stream for channel 12 connected successfully! Press 'q' to exit.\n"
     ]
    }
   ],
   "source": [
    "cap, channel_id_display = get_nvr_capture_and_channel() # Get capture object and channel ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46fa6de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 15:56:09] YOLO model loaded: yolov8n.pt\n",
      "[2025-05-27 15:56:09] CUDA unavailable. Model will use CPU.\n",
      "[2025-05-27 15:56:09] Recognizable classes in model (yolov8n.pt): {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "model, device_str = setup_model_and_logging() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61be4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_map = model.names if hasattr(model, 'names') and model.names else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f069d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_event_log_time = 0 \n",
    "event_log_cooldown = 10 # Seconds, cooldown for logging and screenshotting upon detection in a frame\n",
    "\n",
    "frame_count = [0]\n",
    "process_every_n_frames = 15 # Process every N-th frame (configurable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f7256d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_drawn_annotations = [] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be2ed4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32a48cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG.CLASSES_TO_DETECT = [0, 1, 2, 3, 4, 5, 6,7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38e0e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(\n",
    "    source=frame.copy(), # Pass a copy so the original frame remains clean for screenshot\n",
    "    # save=True, # YOLO will save its images if needed - uncomment\n",
    "    classes=CFG.CLASSES_TO_DETECT, # Filter only necessary classes\n",
    "    conf=CFG.CONFIDENCE,\n",
    "    # save_txt=True, # Save annotations to txt if needed - uncomment\n",
    "    # save_conf=True,\n",
    "    show=False, # Set to False, use cv2.imshow for display\n",
    "    device=device_str, # Use determined device\n",
    "    verbose=False # Less console output from YOLO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e6153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "28225121",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_target_classes_info =[]\n",
    "current_frame_annotations = []\n",
    "if results and results[0] and hasattr(results[0], 'boxes') and len(results[0].boxes) > 0:\n",
    "    for i in range(len(results[0].boxes)):\n",
    "        cls_id = int(results[0].boxes.cls[i])\n",
    "        # Since 'classes' arg filters already, all results will be from this list\n",
    "        conf_score = float(results[0].boxes.conf[i])\n",
    "        bbox_xyxy = results[0].boxes.xyxy[i].cpu().numpy().astype(int)\n",
    "        class_name = class_names_map.get(cls_id, f\"ID:{cls_id}\") # Get class name\n",
    "        timestamp_file = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        info_temp = [class_name, round(conf_score,2), bbox_xyxy,timestamp_file]\n",
    "\n",
    "        detected_target_classes_info.append(info_temp)\n",
    "        \n",
    "        # Prepare information for drawing\n",
    "        px1, py1, px2, py2 = bbox_xyxy\n",
    "        text_to_draw = f\"{class_name} ({conf_score:.2f})\"\n",
    "        current_frame_annotations.append(\n",
    "            (text_to_draw, (px1, py1 - 10), \n",
    "                (px1, py1, px2, py2), \n",
    "                (0, 0,250), 2) # read color for all target objects\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7909b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e3648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20250527_163516_078781'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e0bc5935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>person</td>\n",
       "      <td>0.67</td>\n",
       "      <td>[898, 341, 949, 579]</td>\n",
       "      <td>20250527_163143_475499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>person</td>\n",
       "      <td>0.60</td>\n",
       "      <td>[836, 367, 897, 605]</td>\n",
       "      <td>20250527_163143_475692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1                     2                       3\n",
       "0  person  0.67  [898, 341, 949, 579]  20250527_163143_475499\n",
       "1  person  0.60  [836, 367, 897, 605]  20250527_163143_475692"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detected_target_classes_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "adbc3224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person (conf: 0.67) at [898 341 949 579]',\n",
       " 'person (conf: 0.60) at [836 367 897 605]']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_target_classes_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2a28cc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('person (0.67)',\n",
       "  (np.int64(898), np.int64(331)),\n",
       "  (np.int64(898), np.int64(341), np.int64(949), np.int64(579)),\n",
       "  (250, 0, 0),\n",
       "  2),\n",
       " ('person (0.60)',\n",
       "  (np.int64(836), np.int64(357)),\n",
       "  (np.int64(836), np.int64(367), np.int64(897), np.int64(605)),\n",
       "  (250, 0, 0),\n",
       "  2)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_frame_annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ee45c45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_frame_for_screenshot = frame.copy()\n",
    "for text, text_pos, rect_coords, color, thickness in current_frame_annotations:\n",
    "    cv2.rectangle(temp_frame_for_screenshot, (rect_coords[0], rect_coords[1]), (rect_coords[2], rect_coords[3]), color, thickness)\n",
    "    cv2.putText(temp_frame_for_screenshot, text, text_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, thickness)\n",
    "cv2.imwrite('tt2.jpg', temp_frame_for_screenshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75162fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20250527_162810_601136'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32787879",
   "metadata": {},
   "outputs": [],
   "source": [
    "if detected_target_classes_info:\n",
    "    current_event_time = time.time()\n",
    "    if current_event_time - last_event_log_time > event_log_cooldown:\n",
    "        timestamp_file = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        screenshot_filename = os.path.join(SCREENSHOT_DIR, f\"detected_objects_ch{channel_id_display}_{timestamp_file}.jpg\")\n",
    "        \n",
    "        # To have the screenshot with current detection boxes, draw them on a temp frame before saving\n",
    "        temp_frame_for_screenshot = frame.copy()\n",
    "        for text, text_pos, rect_coords, color, thickness in current_frame_annotations:\n",
    "                cv2.rectangle(temp_frame_for_screenshot, (rect_coords[0], rect_coords[1]), (rect_coords[2], rect_coords[3]), color, thickness)\n",
    "                cv2.putText(temp_frame_for_screenshot, text, text_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, thickness)\n",
    "        cv2.imwrite(screenshot_filename, temp_frame_for_screenshot)\n",
    "        \n",
    "        event_summary = \"; \".join(detected_target_classes_info)\n",
    "        log_message = f\"EVENT (Channel {channel_id_display}): Detected objects - {event_summary}. Screenshot saved: {screenshot_filename}\"\n",
    "        log_event(log_message)\n",
    "        \n",
    "        last_event_log_time = current_event_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d376a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c71aaa9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].boxes.cls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9e8dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('1',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31b73aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ret, frame = cap.read()\n",
    "cv2.imwrite('test.jpg', frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f943738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret, frame2 = cap.read()\n",
    "cv2.imwrite('test2.jpg', frame2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c5cba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('1',frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39c8d994",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:973: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mObject Detection - NVR Channel \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mchannel_id_display\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\0185972\\myenv\\com\\Lib\\site-packages\\ultralytics\\utils\\patches.py:86\u001b[39m, in \u001b[36mimshow\u001b[39m\u001b[34m(winname, mat)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimshow\u001b[39m(winname: \u001b[38;5;28mstr\u001b[39m, mat: np.ndarray):\n\u001b[32m     69\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[33;03m    Display an image in the specified window.\u001b[39;00m\n\u001b[32m     71\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03m        >>> imshow(\"Example Window\", img)  # Display the image\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[43m_imshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwinname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43municode_escape\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:973: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": [
    "cv2.imshow(f'Object Detection - NVR Channel {channel_id_display}', frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc618794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Main program ---\n",
    "def main():\n",
    "    cap, channel_id_display = get_nvr_capture_and_channel() # Get capture object and channel ID\n",
    "    if not cap: # If failed to connect to NVR\n",
    "        return\n",
    "\n",
    "    model, device_str = setup_model_and_logging() # Get model and device string\n",
    "    if not model: # If failed to load model\n",
    "        cap.release() # Release NVR resource\n",
    "        return\n",
    "    \n",
    "    class_names_map = model.names if hasattr(model, 'names') and model.names else {}\n",
    "\n",
    "    last_event_log_time = 0 \n",
    "    event_log_cooldown = 10 # Seconds, cooldown for logging and screenshotting upon detection in a frame\n",
    "\n",
    "    frame_count = [0]\n",
    "    process_every_n_frames = 15 # Process every N-th frame (configurable)\n",
    "    \n",
    "    # To store annotation info from the previously processed frame\n",
    "    # (so that boxes remain visible between processing)\n",
    "    last_drawn_annotations = [] \n",
    "\n",
    "    log_event(f\"Starting video stream processing for channel {channel_id_display}...\")\n",
    "    log_event(f\"Model will detect classes with ID: {CFG.CLASSES_TO_DETECT}\")\n",
    "    log_event(f\"Confidence threshold for detection: {CFG.CONFIDENCE}\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            log_event(\"Error: Failed to read frame.\")\n",
    "            break\n",
    "        \n",
    "        work_1frame(model=model,\n",
    "                    class_names_map=class_names_map,\n",
    "                    frame=frame, \n",
    "                    frame_count=frame_count, \n",
    "                    process_every_n_frames=process_every_n_frames,\n",
    "                    channel_id_display=channel_id_display, \n",
    "                    device_str=device_str,\n",
    "                    event_log_cooldown=event_log_cooldown,\n",
    "                    last_event_log_time=last_event_log_time,\n",
    "                    last_drawn_annotations=last_drawn_annotations\n",
    "                    )\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            log_event(\"'q' key pressed, exiting...\")\n",
    "            break\n",
    "\n",
    "    if cap: # Check if cap was successfully initialized\n",
    "        cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    log_event(\"Program stopped.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a074d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "com",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
